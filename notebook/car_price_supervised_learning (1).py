# -*- coding: utf-8 -*-
"""Car_Price_Supervised_Learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11mb293_nLch7i5-T5CCsKc3Ng1v55H4g

#Libraries
"""

import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler,MinMaxScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score
from sklearn.model_selection import GridSearchCV #model fine tuning
from sklearn.ensemble import RandomForestClassifier  # Model Feature Selection
from sklearn.ensemble import RandomForestRegressor

"""#Load Data"""

from google.colab import drive
drive.mount('/content/drive')

filepath='/content/drive/MyDrive/DSA ICT/Data/car_prediction_data.csv'
car=pd.read_csv(filepath)
car.head()

"""#EDA"""

car.info()

cat_cols = car.select_dtypes(include=['object']).columns
for col in cat_cols:
  print('Coloumn : ',col)
  print(car[col].value_counts(dropna=False))
print('Categorical columns:\n',cat_cols)

"""#Data Cleaning

Handling Duplicates
"""

car.duplicated().sum()

car = car.drop_duplicates()

car.duplicated().sum()

"""Handling Missing Values"""

car.isnull().sum()

"""Handling Outliers"""

num_cols = car.select_dtypes(include=['int64', 'float64']).columns
for col in num_cols:
    plt.figure()
    plt.boxplot(car[col])
    plt.title(f'Box Plot of {col}')
    plt.ylabel(col)
    plt.show()

print(num_cols)

num_cols1=['Year', 'Selling_Price', 'Present_Price', 'Kms_Driven']

"""Outlier Removal(IQR Method)"""

for col in num_cols1:
  q1=np.percentile(car[col],25)
  q3=np.percentile(car[col],75)
  iqr=q3-q1
  print(col,q1,q3,iqr)

  up_val=q3+1.5*iqr
  low_val=q1-1.5*iqr
  print(up_val,low_val)

  car[col]=car[col].clip(lower=low_val,upper=up_val)

for col in num_cols1:
    plt.figure()
    plt.boxplot(car[col])
    plt.title(f'Box Plot of {col}')
    plt.ylabel(col)
    plt.show()

"""#Feature Engineering"""

car1=car.select_dtypes(include=['number'])
corr=car1.corr()
corr

sns.heatmap(corr,annot=True,cmap='coolwarm')

for col in num_cols:
    plt.figure()
    plt.hist(car[col])
    plt.title(f'Histogram of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.show()

"""#Encoding"""

print(cat_cols)

car=car.drop(['Car_Name'],axis=1)

cat_cols1=['Fuel_Type', 'Seller_Type', 'Transmission']

for col in cat_cols1:
  le=LabelEncoder()
  car[col]=le.fit_transform(car[col])

car.head()

"""#Train-Test Split"""

X = car.drop('Selling_Price', axis=1)
y = car['Selling_Price']

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=50)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

"""#Scaling

Min-Max Scaling
"""

ss = MinMaxScaler()
X_train=ss.fit_transform(X_train)
X_test=ss.transform(X_test)

"""#ML Modeling Using Regression Method

#Linear Regression
"""

lin_model=LinearRegression()
lin_model.fit(X_train,y_train)
y_pred=lin_model.predict(X_test)
y_pred

mae=mean_absolute_error(y_test,y_pred)
mse=mean_squared_error(y_test,y_pred)
r2=r2_score(y_test,y_pred)

print(f'Mean Absolute Error: {mae}')
print(f'Mean Squared Error: {mse}')
print(f'r2= {r2}')

"""#Linear Regression Model Fine Tuning using Grid Search CV **(Optional)**"""

param_grid_lr = {
    'fit_intercept': [True, False],
    'positive': [True, False]
}

grid_search_lr = GridSearchCV(estimator=LinearRegression(), param_grid=param_grid_lr, scoring='r2', cv=5, n_jobs=-1)
grid_search_lr.fit(X_train, y_train)

print("Best parameters found: ", grid_search_lr.best_params_)
print("Best R2 score found: ", grid_search_lr.best_score_)

best_lin_model = grid_search_lr.best_estimator_
y_pred_tuned = best_lin_model.predict(X_test)

mae_tuned = mean_absolute_error(y_test, y_pred_tuned)
mse_tuned = mean_squared_error(y_test, y_pred_tuned)
r2_tuned = r2_score(y_test, y_pred_tuned)

print(f'\nMean Absolute Error (Tuned): {mae_tuned}')
print(f'Mean Squared Error (Tuned): {mse_tuned}')
print(f'R2 Score (Tuned): {r2_tuned}')

"""#Modeling Using Random Forest Regressor"""

rf_model = RandomForestRegressor(random_state=50)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

mae_rf = mean_absolute_error(y_test, y_pred_rf)
mse_rf = mean_squared_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)

print(f'Random Forest Mean Absolute Error: {mae_rf}')
print(f'Random Forest Mean Squared Error: {mse_rf}')
print(f'Random Forest R2 Score: {r2_rf}')

"""To fine-tune the RandomForestRegressor model, I need to use GridSearchCV. This requires defining a parameter grid for the Random Forest model and then applying GridSearchCV to find the best combination of hyperparameters.


"""

param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_features': ['sqrt', 'log2'],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search_rf = GridSearchCV(estimator=RandomForestRegressor(random_state=50), param_grid=param_grid_rf, scoring='r2', cv=5, n_jobs=-1, verbose=2)
grid_search_rf.fit(X_train, y_train)

print("Best parameters found: ", grid_search_rf.best_params_)
print("Best R2 score found: ", grid_search_rf.best_score_)

"""
Now that the best hyperparameters for the RandomForestRegressor have been found using GridSearchCV, I will use these parameters to initialize a new model, train it on the `X_train` and `y_train` data, and then make predictions on the `X_test` data. This will allow for evaluation of the fine-tuned model's performance.

"""

best_rf_model = grid_search_rf.best_estimator_
y_pred_rf_tuned = best_rf_model.predict(X_test)
print("Fine-tuned Random Forest Model initialized, trained, and predictions made.")

mae_rf_tuned = mean_absolute_error(y_test, y_pred_rf_tuned)
mse_rf_tuned = mean_squared_error(y_test, y_pred_rf_tuned)
r2_rf_tuned = r2_score(y_test, y_pred_rf_tuned)

print(f'Fine-tuned Random Forest Mean Absolute Error: {mae_rf_tuned}')
print(f'Fine-tuned Random Forest Mean Squared Error: {mse_rf_tuned}')
print(f'Fine-tuned Random Forest R2 Score: {r2_rf_tuned}')

"""Downloading pickle file"""

import pickle
pickle.dump(best_rf_model, open('best_rf_model.pkl', 'wb'))
pickle.dump(ss, open('ss.pkl', 'wb'))
print("Fine-tuned Random Forest model and scaler saved as pickle files.")